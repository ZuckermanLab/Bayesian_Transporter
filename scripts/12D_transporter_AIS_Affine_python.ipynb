{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd685754",
   "metadata": {},
   "source": [
    "# affine invariant ensemble sampler + AIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf5f16",
   "metadata": {},
   "source": [
    "### 12D transporter model (single antiporter cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6224c18",
   "metadata": {},
   "source": [
    "Annealed Importance Sampling w/ the Affine Invariant Sampler\n",
    "\n",
    "1. Initialization\n",
    "    1. set $\\mathrm{ESS_{min}}$, $\\beta=0$, $\\Delta\\beta$, $N$ steps, $K$ walkers, last fraction of samples to keep $\\alpha$ \n",
    "        1. note: choose small $\\Delta\\beta$ \n",
    "        2. note: choose  $(\\alpha \\cdot N\\cdot K)$>>$K$\n",
    "        3. note: need to let samples become 'decorrelated' $\\rightarrow$ select some fraction of samples at the end\n",
    "    2. generate $\\alpha \\cdot N\\cdot K$ initial samples from a Uniform distribution (same as priors)\n",
    "    3. calculate likelihoods at initial sample points ($\\beta$=0), $\\mathcal{L_{0}}$\n",
    "2. Determine new $\\beta$ \n",
    "    1. WHILE $\\mathrm{ESS}$ > $\\mathrm{ESS_{min}}$:\n",
    "        1. increment $\\beta$:  $\\beta_{\\mathrm{new}} \\rightarrow \\beta + \\Delta\\beta$\n",
    "        2. calculate $\\mathrm{ESS}$:\n",
    "            1. calculate weights $w$: $w(x) = \\mathcal{L}^{(\\beta_{\\mathrm{new}}-\\beta)}$\n",
    "                1. note: using log likelihood, so use relative weights:\n",
    "                    1. $\\ln(w) = (\\beta_{\\mathrm{new}}-\\beta) \\ln(\\mathcal{L_{\\mathrm{ref}}})$\n",
    "                    2. $\\ln(w)_{\\mathrm{rel}} = \\ln(w)_{\\mathrm{i}} - \\max{\\ln(w)}$ \n",
    "            2. calculate $\\mathrm{ESS}$:\n",
    "                1. $\\mathrm{ESS} = \\sum{\\frac{w_i}{\\max{w}}} = \\sum(\\exp(\\ln(w)_{\\mathrm{rel}}))$\n",
    "3. Run sampler\n",
    "    1. resample $K$ walkers, drawn with replacement from initial parameter set distribution using weights, $w$.\n",
    "    2. run (affine invarient ensemble) sampler for $N\\cdot K$ total samples\n",
    "    3. select the last fraction of samples to keep $\\alpha \\cdot N\\cdot K$ \n",
    "    4. calculate likelihood for the above samples\n",
    "4. repeat 2-3 until $\\beta=1$\n",
    "    1. (possibly run sampler longer when $\\beta=1$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, tellurium as te, matplotlib.pyplot as plt\n",
    "import emcee as mc, corner, time, pandas as pd\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "# Normal log-likelihood calculation\n",
    "def calc_norm_log_likelihood(mu,sigma,X):\n",
    "    # Normal log-likelihood function: -[(n/2)ln(2pp*sigma^2)]-[sum((X-mu)^2)/(2*sigma^2)]\n",
    "    # ref: https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood \n",
    "    n = len(X)\n",
    "    f1 = -1*(n/2)*np.log(2*np.pi*sigma**2)\n",
    "    f2_a = -1/(2*sigma**2)\n",
    "    f2_b = 0 \n",
    "    for i in range(n):\n",
    "        f2_b += (X[i]-mu[i])**2\n",
    "    f2 = f2_a*f2_b\n",
    "    log_likelihood = f1+f2\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def calc_grid_point(K,y_obs,m):\n",
    "    m.resetToOrigin()\n",
    "    m.H_out_activation = 5e-8\n",
    "    m.integrator.absolute_tolerance = 1e-18\n",
    "    m.integrator.relative_tolerance = 1e-12\n",
    "    m.k1_f = 10**K[0]\n",
    "    m.k1_r = 10**K[1]\n",
    "    m.k6_r = (m.k1_f*m.k2_f*m.k3_f*m.k4_f*m.k5_f*m.k6_f)/(m.k1_r*m.k2_r*m.k3_r*m.k4_r*m.k5_r)\n",
    " \n",
    "    D_tmp = m.simulate(0,15, 376, selections=['time', 'rxn4'])\n",
    "    y_tmp = D_tmp['rxn4']\n",
    "    sigma = 1e-13\n",
    "    log_like_tmp = calc_norm_log_likelihood(y_tmp,sigma,y_obs)\n",
    "    return log_like_tmp\n",
    "\n",
    "\n",
    "antimony_string = f\"\"\"\n",
    "            // Created by libAntimony v2.12.0\n",
    "            model transporter_full()\n",
    "\n",
    "            // Compartments and Species:\n",
    "            compartment vol;\n",
    "            species OF in vol, OF_Hb in vol;\n",
    "            species IF_Hb in vol, IF_Hb_Sb in vol;\n",
    "            species IF_Sb in vol, OF_Sb in vol;\n",
    "            species H_in in vol, S_in in vol;\n",
    "            species $H_out in vol, $S_out in vol;\n",
    "\n",
    "            // Reactions:\n",
    "            rxn1: OF + $H_out -> OF_Hb; vol*(k1_f*OF*H_out - k1_r*OF_Hb);\n",
    "            rxn2: OF_Hb -> IF_Hb; vol*(k2_f*OF_Hb - k2_r*IF_Hb);\n",
    "            rxn3: IF_Hb + S_in -> IF_Hb_Sb; vol*(k3_f*IF_Hb*S_in - k3_r*IF_Hb_Sb);\n",
    "            rxn4: IF_Hb_Sb -> IF_Sb + H_in; vol*(k4_f*IF_Hb_Sb - k4_r*IF_Sb*H_in);\n",
    "            rxn5: IF_Sb -> OF_Sb; vol*(k5_f*IF_Sb - k5_r*OF_Sb);\n",
    "            rxn6: OF_Sb -> OF + $S_out; vol*(k6_f*OF_Sb - k6_r*OF*S_out);\n",
    "            \n",
    "\n",
    "            // Events:\n",
    "            E1: at (time >= 5): H_out = H_out_activation, S_out = S_out_activation;\n",
    "            E2: at (time >= 10): H_out = 1e-7, S_out = 0.001;\n",
    "\n",
    "            // Species initializations:\n",
    "            H_out = 1e-07;\n",
    "            H_out has substance_per_volume;\n",
    "\n",
    "            H_in = 1e-7;\n",
    "            H_in has substance_per_volume;\n",
    "\n",
    "            S_out = 0.001;\n",
    "            S_out has substance_per_volume;\n",
    "\n",
    "            S_in = 1e-3;\n",
    "            S_in has substance_per_volume;\n",
    "\n",
    "            OF = 2.833e-8;\n",
    "            OF has substance_per_volume;\n",
    "\n",
    "            OF_Hb = 2.833e-8;\n",
    "            OF_Hb has substance_per_volume;\n",
    "\n",
    "            IF_Hb = 2.833e-8;\n",
    "            IF_Hb has substance_per_volume;\n",
    "            \n",
    "            IF_Hb_Sb = 2.833e-8;\n",
    "            IF_Hb_Sb has substance_per_volume;\n",
    "            \n",
    "            IF_Sb = 2.125e-08;\n",
    "            IF_Sb has substance_per_volume;\n",
    "\n",
    "            OF_Sb = 2.125e-08;\n",
    "            OF_Sb has substance_per_volume;\n",
    "\n",
    "\n",
    "            // Compartment initializations:\n",
    "            vol = 0.0001;\n",
    "            vol has volume;\n",
    "\n",
    "            // Variable initializations:\n",
    "            H_out_activation = 5e-8;\n",
    "            S_out_activation = 0.001;\n",
    "\n",
    "            // Rate constant initializations:\n",
    "            k1_f = 1e10;\n",
    "            k1_r = 1e3;\n",
    "            k2_f = 1e2;\n",
    "            k2_r = 1e2;\n",
    "            k3_f = 1e7;\n",
    "            k3_r = 1e3;\n",
    "            k4_f = 1e3;\n",
    "            k4_r = 1e10;\n",
    "            k5_f = 1e2;\n",
    "            k5_r = 1e2;\n",
    "            k6_f = 1e3;\n",
    "            k6_r = 1e7;\n",
    "\n",
    "\n",
    "            // Other declarations:\n",
    "            const vol;\n",
    "            const k1_f, k1_r, k2_f, k2_r, k3_f, k3_r;\n",
    "            const k4_f, k4_r, k5_f, k5_r, k6_f, k6_r;\n",
    "    \n",
    "\n",
    "            // Unit definitions:\n",
    "            unit substance_per_volume = mole / litre;\n",
    "            unit volume = litre;\n",
    "            unit length = metre;\n",
    "            unit area = metre^2;\n",
    "            unit time_unit = second;\n",
    "            unit substance = mole;\n",
    "            unit extent = mole;\n",
    "\n",
    "            // Display Names:\n",
    "            time_unit is \"time\";\n",
    "            end\n",
    "            \"\"\" \n",
    "\n",
    "antimony_string3 = f\"\"\"\n",
    "            // Created by libAntimony v2.12.0\n",
    "            model transporter_full()\n",
    "\n",
    "            // Compartments and Species:\n",
    "            compartment vol;\n",
    "            species OF in vol, OF_Hb in vol;\n",
    "            species IF_Hb in vol, IF_Hb_Sb in vol;\n",
    "            species IF_Sb in vol, OF_Sb in vol;\n",
    "            species H_in in vol, S_in in vol;\n",
    "            species $H_out in vol, $S_out in vol;\n",
    "\n",
    "            // Reactions:\n",
    "            rxn1: OF + $H_out -> OF_Hb; vol*(k1_f*OF*H_out - k1_r*OF_Hb);\n",
    "            rxn2: OF_Hb -> IF_Hb; vol*(k2_f*OF_Hb - k2_r*IF_Hb);\n",
    "            rxn3: IF_Hb + S_in -> IF_Hb_Sb; vol*(k3_f*IF_Hb*S_in - k3_r*IF_Hb_Sb);\n",
    "            rxn4: IF_Hb_Sb -> IF_Sb + H_in; vol*(k4_f*IF_Hb_Sb - k4_r*IF_Sb*H_in);\n",
    "            rxn5: IF_Sb -> OF_Sb; vol*(k5_f*IF_Sb - k5_r*OF_Sb);\n",
    "            rxn6: OF_Sb -> OF + $S_out; vol*(k6_f*OF_Sb - k6_r*OF*S_out);\n",
    "            \n",
    "\n",
    "            // Events:\n",
    "            E1: at (time >= 5): H_out = 1e-7, S_out = 1e-3;\n",
    "            \n",
    "\n",
    "            // Species initializations:\n",
    "            H_out = 5e-8;\n",
    "            H_out has substance_per_volume;\n",
    "\n",
    "            H_in = 9.999811082242941e-08;\n",
    "            H_in has substance_per_volume;\n",
    "\n",
    "            S_out = 0.001;\n",
    "            S_out has substance_per_volume;\n",
    "\n",
    "            S_in = 0.0009999811143288836;\n",
    "            S_in has substance_per_volume;\n",
    "\n",
    "            OF = 4.7218452046117796e-09;\n",
    "            OF has substance_per_volume;\n",
    "\n",
    "            OF_Hb = 4.7218452046117796e-09;\n",
    "            OF_Hb has substance_per_volume;\n",
    "\n",
    "            IF_Hb = 4.7218452046117796e-09;\n",
    "            IF_Hb has substance_per_volume;\n",
    "            \n",
    "            IF_Hb_Sb = 4.721756029392908e-08;\n",
    "            IF_Hb_Sb has substance_per_volume;\n",
    "            \n",
    "            IF_Sb = 4.721845204611779e-08;\n",
    "            IF_Sb has substance_per_volume;\n",
    "\n",
    "            OF_Sb = 4.721845204611775e-08;\n",
    "            OF_Sb has substance_per_volume;\n",
    "\n",
    "\n",
    "            // Compartment initializations:\n",
    "            vol = 0.0001;\n",
    "            vol has volume;\n",
    "\n",
    "            // Rate constant initializations:\n",
    "            k1_f = 1e10;\n",
    "            k1_r = 1e3;\n",
    "            k2_f = 1e2;\n",
    "            k2_r = 1e2;\n",
    "            k3_f = 1e7;\n",
    "            k3_r = 1e3;\n",
    "            k4_f = 1e3;\n",
    "            k4_r = 1e10;\n",
    "            k5_f = 1e2;\n",
    "            k5_r = 1e2;\n",
    "            k6_f = 1e3;\n",
    "            k6_r = 1e7;\n",
    "\n",
    "\n",
    "            // Other declarations:\n",
    "            const vol;\n",
    "            const k1_f, k1_r, k2_f, k2_r, k3_f, k3_r;\n",
    "            const k4_f, k4_r, k5_f, k5_r, k6_f, k6_r;\n",
    "    \n",
    "\n",
    "            // Unit definitions:\n",
    "            unit substance_per_volume = mole / litre;\n",
    "            unit volume = litre;\n",
    "            unit length = metre;\n",
    "            unit area = metre^2;\n",
    "            unit time_unit = second;\n",
    "            unit substance = mole;\n",
    "            unit extent = mole;\n",
    "\n",
    "            // Display Names:\n",
    "            time_unit is \"time\";\n",
    "            end\n",
    "            \"\"\" \n",
    "\n",
    "\n",
    "antimony_string2 = f\"\"\"\n",
    "            // Created by libAntimony v2.12.0\n",
    "            model transporter_full()\n",
    "\n",
    "            // Compartments and Species:\n",
    "            compartment vol;\n",
    "            species OF in vol, OF_Hb in vol;\n",
    "            species IF_Hb in vol, IF_Hb_Sb in vol;\n",
    "            species IF_Sb in vol, OF_Sb in vol;\n",
    "            species H_in in vol, S_in in vol;\n",
    "            species $H_out in vol, $S_out in vol;\n",
    "\n",
    "            // Reactions:\n",
    "            rxn1: OF + $H_out -> OF_Hb; vol*(k1_f*OF*H_out - k1_r*OF_Hb);\n",
    "            rxn2: OF_Hb -> IF_Hb; vol*(k2_f*OF_Hb - k2_r*IF_Hb);\n",
    "            rxn3: IF_Hb + S_in -> IF_Hb_Sb; vol*(k3_f*IF_Hb*S_in - k3_r*IF_Hb_Sb);\n",
    "            rxn4: IF_Hb_Sb -> IF_Sb + H_in; vol*(k4_f*IF_Hb_Sb - k4_r*IF_Sb*H_in);\n",
    "            rxn5: IF_Sb -> OF_Sb; vol*(k5_f*IF_Sb - k5_r*OF_Sb);\n",
    "            rxn6: OF_Sb -> OF + $S_out; vol*(k6_f*OF_Sb - k6_r*OF*S_out);\n",
    "            \n",
    "\n",
    "            // Species initializations:\n",
    "            H_out = 1e-07;\n",
    "            H_out has substance_per_volume;\n",
    "\n",
    "            H_in = 1e-7;\n",
    "            H_in has substance_per_volume;\n",
    "\n",
    "            S_out = 0.001;\n",
    "            S_out has substance_per_volume;\n",
    "\n",
    "            S_in = 1e-3;\n",
    "            S_in has substance_per_volume;\n",
    "\n",
    "            OF = 2.833e-8;\n",
    "            OF has substance_per_volume;\n",
    "\n",
    "            OF_Hb = 2.833e-8;\n",
    "            OF_Hb has substance_per_volume;\n",
    "\n",
    "            IF_Hb = 2.833e-8;\n",
    "            IF_Hb has substance_per_volume;\n",
    "            \n",
    "            IF_Hb_Sb = 2.833e-8;\n",
    "            IF_Hb_Sb has substance_per_volume;\n",
    "            \n",
    "            IF_Sb = 2.125e-08;\n",
    "            IF_Sb has substance_per_volume;\n",
    "\n",
    "            OF_Sb = 2.125e-08;\n",
    "            OF_Sb has substance_per_volume;\n",
    "\n",
    "\n",
    "            // Compartment initializations:\n",
    "            vol = 0.0001;\n",
    "            vol has volume;\n",
    "\n",
    "\n",
    "            // Rate constant initializations:\n",
    "            k1_f = 1e10;\n",
    "            k1_r = 1e3;\n",
    "            k2_f = 1e2;\n",
    "            k2_r = 1e2;\n",
    "            k3_f = 1e7;\n",
    "            k3_r = 1e3;\n",
    "            k4_f = 1e3;\n",
    "            k4_r = 1e10;\n",
    "            k5_f = 1e2;\n",
    "            k5_r = 1e2;\n",
    "            k6_f = 1e3;\n",
    "            k6_r = 1e7;\n",
    "\n",
    "\n",
    "            // Other declarations:\n",
    "            const vol;\n",
    "            const k1_f, k1_r, k2_f, k2_r, k3_f, k3_r;\n",
    "            const k4_f, k4_r, k5_f, k5_r, k6_f, k6_r;\n",
    "    \n",
    "\n",
    "            // Unit definitions:\n",
    "            unit substance_per_volume = mole / litre;\n",
    "            unit volume = litre;\n",
    "            unit length = metre;\n",
    "            unit area = metre^2;\n",
    "            unit time_unit = second;\n",
    "            unit substance = mole;\n",
    "            unit extent = mole;\n",
    "\n",
    "            // Display Names:\n",
    "            time_unit is \"time\";\n",
    "            end\n",
    "            \"\"\" \n",
    "\n",
    "\n",
    "\n",
    "m = te.loada(antimony_string)\n",
    "m.integrator.absolute_tolerance = 1e-18\n",
    "m.integrator.relative_tolerance = 1e-12\n",
    "\n",
    "\n",
    "\n",
    "D = m.simulate(0, 15, 376, selections=['time', 'rxn4'])\n",
    "#y_true = D['rxn4']\n",
    "\n",
    "m.resetToOrigin()\n",
    "m.H_out_activation = 2e-7\n",
    "m.integrator.absolute_tolerance = 1e-18\n",
    "m.integrator.relative_tolerance = 1e-12\n",
    "D2 = m.simulate(0, 15, 376, selections=['time', 'rxn4'])\n",
    "\n",
    "\n",
    "\n",
    "y_true = np.hstack([D['rxn4'],D2['rxn4']])\n",
    "\n",
    "noise_stdev_true = 1e-13\n",
    "y_obs = y_true + np.random.normal(0, noise_stdev_true, len(y_true))\n",
    "#y_obs = np.genfromtxt(\"data_grid_test3_2.csv\", skip_header=1)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(y_obs, 'o', alpha=0.25)\n",
    "plt.plot(y_true)\n",
    "plt.ylim(-2.5e-11, 2.5e-11)\n",
    "\n",
    "log_like_ref = calc_norm_log_likelihood(y_true,noise_stdev_true,y_obs)\n",
    "\n",
    "print(log_like_ref)\n",
    "\n",
    "m2 = te.loada(antimony_string3)\n",
    "m2.integrator.absolute_tolerance = 1e-18\n",
    "m2.integrator.relative_tolerance = 1e-12\n",
    "m2.H_out = 5e-8\n",
    "D3 = m2.simulate(0, 10, 250, selections=['time', 'rxn4'])\n",
    "m2.resetToOrigin()\n",
    "m2.H_out = 2e-7\n",
    "m2.integrator.absolute_tolerance = 1e-18\n",
    "m2.integrator.relative_tolerance = 1e-12\n",
    "D4 = m2.simulate(0, 10, 250, selections=['time', 'rxn4'])\n",
    "y_true2 = np.hstack([D3['rxn4'],D4['rxn4']])\n",
    "y_obs2 = y_true2 + np.random.normal(0, noise_stdev_true, len(y_true2))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(y_obs2, 'o', alpha=0.25)\n",
    "plt.plot(y_true2)\n",
    "plt.ylim(-2.5e-11, 2.5e-11)\n",
    "np.savetxt(\"data_grid_test3_2exp.csv\", y_obs2, delimiter=\",\")\n",
    "\n",
    "log_like_ref = calc_norm_log_likelihood(y_true2,noise_stdev_true,y_obs2)\n",
    "print(log_like_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24e32f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Algorithm outline: \n",
    "\n",
    "### Functions\n",
    "\n",
    "# Normal log-likelihood calculation\n",
    "def calc_norm_log_likelihood(mu,sigma,X):\n",
    "    # Normal log-likelihood function: -[(n/2)ln(2pp*sigma^2)]-[sum((X-mu)^2)/(2*sigma^2)]\n",
    "    # ref: https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood \n",
    "    n = len(X)\n",
    "    f1 = -1*(n/2)*np.log(2*np.pi*sigma**2)\n",
    "    f2_a = -1/(2*sigma**2)\n",
    "    f2_b = 0 \n",
    "    for i in range(n):\n",
    "        f2_b += (X[i]-mu[i])**2\n",
    "    f2 = f2_a*f2_b\n",
    "    log_likelihood = f1+f2\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "# simulate model and then compare flux data to observed data\n",
    "def log_likelihood(theta, y_obs, extra_parameters):\n",
    "    '''log of Guassian likelihood distribution'''\n",
    "    sigma = 10**theta[0]\n",
    "    K=theta[1:]\n",
    "    m = extra_parameters[0] \n",
    "    m.resetToOrigin()\n",
    "    m.H_out_activation = 5e-8\n",
    "    m.integrator.absolute_tolerance = 1e-18\n",
    "    m.integrator.relative_tolerance = 1e-12\n",
    "    m.k1_f = 10**K[0]\n",
    "    m.k1_r = 10**K[1]\n",
    "    m.k2_f = 10**K[2]\n",
    "    m.k2_r = 10**K[3]\n",
    "    m.k3_f = 10**K[4]\n",
    "    m.k3_r = 10**K[5]\n",
    "    m.k4_f = 10**K[6]\n",
    "    m.k4_r = 10**K[7]\n",
    "    m.k5_f = 10**K[8]\n",
    "    m.k5_r = 10**K[9]\n",
    "    m.k6_f = 10**K[10]\n",
    "    m.k6_r = (m.k1_f*m.k2_f*m.k3_f*m.k4_f*m.k5_f*m.k6_f)/(m.k1_r*m.k2_r*m.k3_r*m.k4_r*m.k5_r)\n",
    "    try:\n",
    "        D_tmp = m.simulate(0,15, 376, selections=['time', 'rxn4'])\n",
    "        y_tmp = D_tmp['rxn4']\n",
    "        log_like_tmp = calc_norm_log_likelihood(y_tmp,sigma,y_obs)\n",
    "    except:\n",
    "        log_like_tmp = -np.inf\n",
    "    return log_like_tmp\n",
    "\n",
    "\n",
    "# calculate uniform prior\n",
    "def log_prior(theta):\n",
    "    '''log of uniform prior distribution'''\n",
    "    p0 = theta[0]\n",
    "    p1 = theta[1]\n",
    "    p2 = theta[2]\n",
    "    p3 = theta[3]\n",
    "    p4 = theta[4]\n",
    "    p5 = theta[5]\n",
    "    p6 = theta[6]\n",
    "    p7 = theta[7]\n",
    "    p8 = theta[8]\n",
    "    p9 = theta[9]\n",
    "    p10 = theta[10]\n",
    "    p11 = theta[11]\n",
    "\n",
    "    # if prior is between boundary --> log(prior) = 0 (uninformitive prior)\n",
    "    if (np.log10(5e-14)<p0<np.log10(5e-13)) and (6<p1<12) and (-1<p2<5) and (-2<p3<4) and (-2<p4<4) and \\\n",
    "        (3<p5<9) and (-1<p6<5) and (-1<p7<5) and (6<p8<12) and (-2<p9<4) and (-2<p10<4) and (-1<p11<5):\n",
    "        return 0  \n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "    \n",
    "# calculate log probability (log likelihood + log prior)\n",
    "def log_probability(theta, y_obs, extra_parameters):\n",
    "    '''log of estimated posterior probability'''\n",
    "    B = extra_parameters[1]  # beta term\n",
    "    log_pr = log_prior(theta)\n",
    "    if not np.isfinite(log_pr):\n",
    "        return -np.inf  # ~zero probability\n",
    "    log_like = log_likelihood(theta, y_obs, extra_parameters)**B\n",
    "    log_post = log_pr + log_like\n",
    "    return log_post\n",
    "\n",
    "\n",
    "# set initial parameters\n",
    "def set_p0():\n",
    "    log_noise_sigma = np.random.uniform(np.log10(5e-14), np.log10(5e-13))\n",
    "    log_k1_f = np.random.uniform(6, 12) # log10 rate constant (ref=1e10)\n",
    "    log_k1_r = np.random.uniform(-1,5)  # log10 rate constant (ref=1e3) \n",
    "    log_k2_f = np.random.uniform(-2,4)  # log10 rate constant (ref=1e2)\n",
    "    log_k2_r = np.random.uniform(-2,4)  # log10 rate constant (ref=1e2)\n",
    "\n",
    "    log_k3_f = np.random.uniform(3,9)  # log10 rate constant (ref=1e7) \n",
    "    log_k3_r = np.random.uniform(-1,5)  # log10 rate constant (ref=1e3) \n",
    "    log_k4_f = np.random.uniform(-1,5)  # log10 rate constant (ref=1e3) \n",
    "    log_k4_r = np.random.uniform(6, 12)  # log10 rate constant (ref=1e10)\n",
    "    log_k5_f = np.random.uniform(-2,4)  # log10 rate constant (ref=1e2)\n",
    "    log_k5_r = np.random.uniform(-2,4)   # log10 rate constant (ref=1e2)\n",
    "    log_k6_f = np.random.uniform(-1,5)  # log10  rate constant (ref=1e3)\n",
    "    \n",
    "    p0_list_tmp = [\n",
    "                log_noise_sigma ,\n",
    "                log_k1_f ,\n",
    "                log_k1_r ,\n",
    "                log_k2_f ,\n",
    "                log_k2_r ,\n",
    "                log_k3_f , \n",
    "                log_k3_r ,\n",
    "                log_k4_f ,\n",
    "                log_k4_r ,\n",
    "                log_k5_f ,\n",
    "                log_k5_r ,\n",
    "                log_k6_f ,\n",
    "    ]\n",
    "    return p0_list_tmp\n",
    "\n",
    "\n",
    "# calculate a new beta \n",
    "def calc_beta(beta_old, d_beta, log_like_ref):\n",
    "    \n",
    "    beta_new = beta_old + d_beta\n",
    "    # figure out converting from log likelihood to weights\n",
    "    \n",
    "    \n",
    "    w_new = (np.sign(log_like_ref))*(np.abs(log_like_ref)**(beta_new-beta_old))\n",
    "    \n",
    "    print(beta_old)\n",
    "    print(d_beta)\n",
    "    print(beta_new)\n",
    "    print(w_new[:2])\n",
    "    pass\n",
    "\n",
    "\n",
    "def calc_weights(log_like_ref, beta1, beta2):\n",
    "    log_w = (beta2-beta1)*log_like_ref  # natural log\n",
    "    print(log_w)\n",
    "    \n",
    "\n",
    "### info for parameters: name, log10 lower bound, log10 upper bound, reference value\n",
    "p_info = [\n",
    "    [\"log_sigma\",np.log10(5e-14), np.log10(5e-13), -13],\n",
    "    [\"log_k1_f\",6,12,10],\n",
    "    [\"log_k1_r\",-1,5,3],\n",
    "    [\"log_k2_f\",-2,4,2],\n",
    "    [\"log_k2_r\",-2,4,2],\n",
    "    [\"log_k3_f\",3,9,7],\n",
    "    [\"log_k3_r\",-1,5,3],\n",
    "    [\"log_k4_f\",-1,5,3],\n",
    "    [\"log_k4_r\",6,12,10],\n",
    "    [\"log_k5_f\",-2,4,2],\n",
    "    [\"log_k5_r\",-2,4,2],\n",
    "    [\"log_k6_f\",-1,5,3],\n",
    "]\n",
    "\n",
    "### Initialize ESS_min, β=0, Δβ, N samples\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "ESS_min = 100\n",
    "beta = 1\n",
    "d_beta = 0.1\n",
    "N_samples = 120\n",
    "extra_parameters = [m,beta]\n",
    "\n",
    "### reference log likelihood\n",
    "p_ref = [-13,10,3,2,2,7,3,3,10,2,2,3]  # log10 parameter values (sigma, k's)\n",
    "print(f\"ref log likelihood: {log_likelihood(p_ref,y_obs,extra_parameters)}\")\n",
    "\n",
    "### initial sample at beta = 0\n",
    "# Sample from uniform likelihood: L(D|θ)^0\n",
    "# Latin hypercube, low-discrepancy sequence ?\n",
    "N_dim = 12\n",
    "N_steps = 5000\n",
    "N_walkers = N_samples\n",
    "\n",
    "pos_list = []\n",
    "for i in range(N_walkers):\n",
    "    p0_list_tmp = set_p0()\n",
    "    pos_list.append(p0_list_tmp)\n",
    "start_pos = np.asarray(pos_list)\n",
    "\n",
    "sampler = mc.EnsembleSampler(N_walkers, N_dim, log_probability,  args=(y_obs, extra_parameters))\n",
    "t_0 = time.time()\n",
    "pos, lnprob, rstate = sampler.run_mcmc(start_pos, n_steps)\n",
    "t_run = time.time() - t_0\n",
    "lp = sampler.lnprobability\n",
    "samples = [np.transpose(sampler.flatchain[0,:])]\n",
    "print(f\"beta: {beta}\")\n",
    "print(f\"wall clock: {t_run} s\")\n",
    "print(f\"{np.shape(lp)[0]*np.shape(lp)[1]/t_run} likelihood calculations / sec \" )\n",
    "\n",
    "df = pd.DataFrame(sampler.flatchain[:,:], columns=[p_info[i][0] for i in range(len(p_info))])\n",
    "df[\"lp\"] = np.array(lp).flatten()\n",
    "print(df)\n",
    "beta=0\n",
    "\n",
    "calc_beta(beta,d_beta, df.iloc[:,12].values)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate β_new such that ESS(β_new) > ESS_min\n",
    "# β_new = β + Δβ\n",
    "# Calculate weights: w ~ L^(β_new – β). [ratio of Gaussian distributions ]\n",
    "# Calculate ESS: sum(weights)/max(weights)\n",
    "# Resample N points using weights from β_new \n",
    "# Run sampler (i.e. affine invariant ensemble) from resampled points\n",
    "# Repeat steps 3 to 5 until β=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7ddae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1D posterior plot\n",
    "fig, ax = plt.subplots(3,4, figsize=(20,15))\n",
    "axes = ax.flatten()\n",
    "\n",
    "for i, ax_i in enumerate(axes):\n",
    "    p_tmp = p_info[i]\n",
    "    ax_i.set_title(f\"{p_tmp[0]}\")\n",
    "    ax_i.axvline(p_tmp[3],ls='--', color='black', alpha=0.5)\n",
    "    ax_i.set_xlim(p_tmp[1],p_tmp[2])\n",
    "    \n",
    "\n",
    "    ax_i.hist(df.iloc[:, i], 100, histtype=\"step\", density=True, range=(p_tmp[1],p_tmp[2]), label=f'{p_tmp[0]}')\n",
    "    ax_i.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "#plt.savefig(f'12D_transporter_AIES_PT_1D_dist_s{seed}_random.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096c4bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"affine_2exp_p0_max_nw_500_ns_100000_data.csv\", usecols=[i+1 for i in range(N_dim)])\n",
    "df2 = pd.read_csv(\"affine_2exp_p0_max_nw_500_ns_100000_s11_data.csv\", usecols=[i+1 for i in range(N_dim)])\n",
    "\n",
    "import corner\n",
    "p_info = [\n",
    "    [\"log_sigma\",np.log10(5e-14), np.log10(5e-13), -13],\n",
    "    [\"log_k1_f\",6,12,10],\n",
    "    [\"log_k1_r\",-1,5,3],\n",
    "    [\"log_k2_f\",-2,4,2],\n",
    "    [\"log_k2_r\",-2,4,2],\n",
    "    [\"log_k3_f\",3,9,7],\n",
    "    [\"log_k3_r\",-1,5,3],\n",
    "    [\"log_k4_f\",-1,5,3],\n",
    "    [\"log_k4_r\",6,12,10],\n",
    "    [\"log_k5_f\",-2,4,2],\n",
    "    [\"log_k5_r\",-2,4,2],\n",
    "    [\"log_k6_f\",-1,5,3],\n",
    "]\n",
    "\n",
    "ranges = [(p_info[i][1],p_info[i][2]) for i in range(len(p_info))]\n",
    "truths = [p_info[i][3] for i in range(len(p_info))]\n",
    "labels = [p_info[i][0] for i in range(len(p_info))]\n",
    "\n",
    "fig = corner.corner(df, bins=200, range=ranges, truths=truths, titles=labels, show_titles=True)\n",
    "savefig('12d_transporter_AIS_PT_pair_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69896533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AIS_Affine_Sampler:\n",
    "    def __init__(self, ESS_min, d_beta, N_steps, K_walkers, alpha, beta_schedule, verbose):\n",
    "        self.ESS_min = float(ESS_min)\n",
    "        self.d_beta = float(d_beta)\n",
    "        self.N_steps = int(N_steps)\n",
    "        self.K_walkers = int(K_walkers)\n",
    "        self.alpha = int(alpha)\n",
    "        self.beta_schedule = beta_schedule\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.M_samples = int(self.N_steps*self.K_walkers)\n",
    "        self.S_subsamples = int(self.alpha*self.K_walkers)\n",
    "        \n",
    "        # check if using adaptive beta schedule or fixed beta schedule\n",
    "        if not self.beta_schedule:\n",
    "            if self.verbose == True:\n",
    "                print('> beta schedule is empty - using adaptive method')\n",
    "            self.beta_adapt = True\n",
    "        else:\n",
    "            self.beta_adapt =False\n",
    "        \n",
    "        # check if hyper parameters are valid\n",
    "        assert(self.M_samples > self.S_subsamples >= 10*self.K_walkers), (\"invalid number of subsamples\")\n",
    "        assert(0.1>self.d_beta<1), (\"invalid delta beta\")\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "ESS_min = 10\n",
    "beta_schedule = []\n",
    "d_beta = 0.01\n",
    "N_steps = 100\n",
    "K_walkers = 10\n",
    "alpha = 10\n",
    "verbose = True\n",
    "\n",
    "sampler = AIS_Affine_Sampler(ESS_min,d_beta, N_steps, K_walkers, alpha, beta_schedule, verbose)\n",
    "print(vars(sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607b326",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### bimodal Gaussian in 1D\n",
    "\n",
    "def prior(x):\n",
    "    if np.all(x>=-20) and np.all(x<=20):\n",
    "        return np.ones(np.size(x))\n",
    "    else:\n",
    "        return np.zeros(np.size(x))\n",
    "\n",
    "    \n",
    "def likelihood(x, mu, sigma):\n",
    "    L = 0.5*(1/(sigma[0] * np.sqrt(2 * np.pi))*np.exp(-(x-mu[0])**2 / (2 * sigma[0]**2))) + 0.5*(1/(sigma[1] * np.sqrt(2 * np.pi))*np.exp(-(x-mu[1])**2 / (2 * sigma[1]**2)))\n",
    "    return L\n",
    "\n",
    "\n",
    "def log_inf(x):\n",
    "    return np.where(x != 0, np.log(x), -np.inf) \n",
    "\n",
    "\n",
    "def log_prob(x, beta, mu, sigma):\n",
    "    pr_x = prior(x) \n",
    "    like_x = likelihood(x, mu, sigma)\n",
    "    log_pr = log_inf(pr_x)\n",
    "    log_like = log_inf(like_x)  \n",
    "    return log_pr + beta*log_like\n",
    "### \n",
    "\n",
    "def OVL(pdf1, pdf2):\n",
    "    # calculate overlapping coefficient\n",
    "    assert(len(pdf1)==len(pdf2))\n",
    "    ovl_coeff = 0\n",
    "    for i in range(len(pdf1)):\n",
    "        ovl_coeff += np.min(pdf1[i], pdf2[i])\n",
    "    return ovl_coeff\n",
    "        \n",
    "\n",
    "### analytical model\n",
    "def prob(x,beta, mu, sigma):\n",
    "    pr = 1.0\n",
    "    L = 0.5*(1/(sigma[0] * np.sqrt(2 * np.pi))*np.exp(-(x-mu[0])**2 / (2 * sigma[0]**2))) + 0.5*(1/(sigma[1] * np.sqrt(2 * np.pi))*np.exp(-(x-mu[1])**2 / (2 * sigma[1]**2)))\n",
    "    \n",
    "    p = pr*(L**beta)\n",
    "    bin_width = np.size(x)/np.ptp(x)\n",
    "    p_sum = np.array([])\n",
    "    \n",
    "    return (pr*(L**beta))\n",
    "\n",
    "### model configuration\n",
    "mu = [-10,10]\n",
    "sigma = [1,2]\n",
    "x = np.linspace(-20,20,100)\n",
    "beta = [0, 0.1, 1]\n",
    "#y_beta = [np.exp(log_prob(x, b, mu, sigma)) for b in beta]  # problem here\n",
    "\n",
    "\n",
    "### sampling settings\n",
    "np.random.seed(10)\n",
    "dim = 1\n",
    "max_iter = 100\n",
    "ESS_min = 500\n",
    "\n",
    "beta=0\n",
    "d_beta = 0.01\n",
    "N_steps = 10000\n",
    "K_walkers = 100\n",
    "M_sub_samples = 10*K_walkers\n",
    "NK_total_samples = N_steps*K_walkers\n",
    "\n",
    "\n",
    "### step 1: initialization\n",
    "S = (np.ptp(x)*np.random.rand(alpha))-np.max(x)  \n",
    "\n",
    "### main loop\n",
    "j = 0\n",
    "while (beta< 1) and (j < max_iter):\n",
    "    assert(np.size(S)==M_sub_samples)\n",
    "    i = 0\n",
    "    S = S.flatten()\n",
    "    ESS = np.inf\n",
    "    log_p1 = log_prob(S,beta,mu,sigma)\n",
    "    \n",
    "    beta_tmp_list = []\n",
    "    log_p2_tmp_list = []\n",
    "    w_rel_tmp_list = []\n",
    "    ESS_tmp_list = []\n",
    "    \n",
    "    beta_new = beta\n",
    "    while (beta_new <= 1) and (ESS > ESS_min) and (i < max_iter):\n",
    "        beta_new = beta+d_beta*i\n",
    "        log_p2 = log_prob(S,beta_new,mu, sigma)\n",
    "        log_w = log_p2 - log_p1\n",
    "        log_w_rel = log_w-np.max(log_w)\n",
    "        w_rel = np.exp(log_w_rel)\n",
    "        p_rel = w_rel/np.sum(w_rel)\n",
    "        ESS = np.sum(w_rel)\n",
    "        print(f'average relative probability: {np.mean(p_rel)}')\n",
    "        beta_tmp_list.append(beta_new)\n",
    "        log_p2_tmp_list.append(log_p2)\n",
    "        w_rel_tmp_list.append(p_rel)\n",
    "        ESS_tmp_list.append(ESS)\n",
    "        print(f'beta new = {beta_new}')\n",
    "        print(f'~ESS (sum of relative probability) = {ESS} samples')\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    beta=beta_tmp_list[-2]\n",
    "    rel_w_list = w_rel_tmp_list[-2]\n",
    "\n",
    "    re_S = np.random.choice(S,size=K_walkers,p=rel_w_list)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    #plt.hist(rel_w_list, 100, range=(-20,20), color=\"k\", histtype=\"step\", density=True, label='rel w')\n",
    "\n",
    "    plt.bar(np.linspace(-20,20,1000), w_rel, color=\"k\", label='rel w', alpha=0.1)\n",
    "    plt.title(f'beta={beta}')\n",
    "    plt.hist(re_S, 100, range=(-20,20), color=\"blue\", histtype=\"step\", density=True, label='resample', alpha=0.5)\n",
    "\n",
    "    start_pos = np.array([[s] for s in re_S])\n",
    "   \n",
    "\n",
    "    sampler = emcee.EnsembleSampler(K_walkers, dim, log_prob, args=[beta, mu, sigma])\n",
    "    state = sampler.run_mcmc(start_pos, N_steps)\n",
    "    S = sampler.flatchain[-alpha:]\n",
    "    \n",
    "    \n",
    "    plt.hist(S, 100, range=(-20,20), color=\"green\", histtype=\"step\", density=True, label='samples', alpha=0.5)\n",
    "   \n",
    "\n",
    "    # reference - affine sampler\n",
    "    p0 = (np.ptp(x)*np.random.rand(K_walkers, dim))-np.max(x)\n",
    "    sampler = emcee.EnsembleSampler(K_walkers, dim, log_prob, args=[beta, mu, sigma])\n",
    "    state = sampler.run_mcmc(p0, N_steps)\n",
    "    samples = sampler.flatchain[burn_in:]\n",
    "    plt.hist(samples, 100, range=(-20,20), color=\"k\", histtype=\"step\", density=True, label='pdf')\n",
    "    plt.legend()\n",
    "    j += 1\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff0e7f",
   "metadata": {},
   "source": [
    "# bimodal Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea86bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "### bimodal Gaussian in 1D\n",
    "\n",
    "def prior(x):\n",
    "    if np.all(x>=-20) and np.all(x<=20):\n",
    "        return np.ones(np.size(x))\n",
    "    else:\n",
    "        return np.zeros(np.size(x))\n",
    "\n",
    "    \n",
    "def likelihood(x, mu, sigma):\n",
    "    L = 0.5*(1/(sigma[0] * np.sqrt(2 * np.pi))*np.exp(-(x-mu[0])**2 / (2 * sigma[0]**2))) + 0.5*(1/(sigma[1] * np.sqrt(2 * np.pi))*np.exp(-(x-mu[1])**2 / (2 * sigma[1]**2)))\n",
    "    return L\n",
    "\n",
    "\n",
    "def log_inf(x):\n",
    "    return np.where(x != 0, np.log(x), -np.inf) \n",
    "\n",
    "\n",
    "def log_prob(x, beta, mu, sigma):\n",
    "    pr_x = prior(x) \n",
    "    like_x = likelihood(x, mu, sigma)\n",
    "    log_pr = log_inf(pr_x)\n",
    "    log_like = log_inf(like_x)\n",
    "    log_post = log_pr + beta*log_like\n",
    "    return log_post\n",
    "### \n",
    "\n",
    "def OVL(hist1, hist2, edges1, edges2):\n",
    "    # calculate overlapping coefficient\n",
    "    assert(len(hist1)==len(hist2))\n",
    "    assert(np.array_equal(edges1,edges2))\n",
    "    \n",
    "    \n",
    "    bin_width = edges1[1]-edges1[0]\n",
    "    ovl_coeff = 0.0\n",
    "    for i in range(len(hist1)):\n",
    "        ovl_coeff += min(bin_width*hist1[i],bin_width*hist2[i])\n",
    "    return ovl_coeff\n",
    "        \n",
    "\n",
    "### analytical model\n",
    "def prob(x,beta, mu, sigma):\n",
    "    pr = 1.0\n",
    "    L = 0.5*(1/(sigma[0] * np.sqrt(2 * np.pi))*np.exp(-(x-mu[0])**2 / (2 * sigma[0]**2))) + 0.5*(1/(sigma[1] * np.sqrt(2 * np.pi))*np.exp(-(x-mu[1])**2 / (2 * sigma[1]**2)))\n",
    "    p = pr*(L**beta)\n",
    "    bin_width = x[1]-x[0]\n",
    "    \n",
    "    return (pr*(L**beta)*(bin_width/np.size(x)))\n",
    "\n",
    "\n",
    "### model configuration\n",
    "mu = [-10,10]\n",
    "sigma = [1,2]\n",
    "x = np.linspace(-20,20,100)\n",
    "beta = [0, 0.1, 1]\n",
    "#y_beta = [np.exp(log_prob(x, b, mu, sigma)) for b in beta]  # problem here\n",
    "y_true = prob(x,1, mu, sigma)\n",
    "print(sum(y_true))\n",
    "\n",
    "plt.plot(x,y_true)\n",
    "plt.show()\n",
    "\n",
    "### sampling settings\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "dim = 1\n",
    "max_iter = 100\n",
    "ESS_min = 500\n",
    "\n",
    "d_beta = 0.01\n",
    "N_steps = 100\n",
    "K_walkers = 10000\n",
    "NK_total_samples = N_steps*K_walkers\n",
    "burn_in = 1000\n",
    "\n",
    "M_sub_samples = NK_total_samples - burn_in #10*K_walkers\n",
    "assert((M_sub_samples < NK_total_samples) & (M_sub_samples >= 10*K_walkers))\n",
    "\n",
    "\n",
    "print(f'random seed: {seed}')\n",
    "print(f'n dim: {dim}')\n",
    "print(f'max iterations: {max_iter}')\n",
    "print(f'ESS min: {ESS_min}')\n",
    "print(f'd Beta: {d_beta}')\n",
    "print(f'N steps: {N_steps}')\n",
    "print(f'K walkers: {K_walkers}')\n",
    "print(f'M sub samples: {M_sub_samples}')\n",
    "print(f'NK total samples: {NK_total_samples}')\n",
    "print(f'burn in: {burn_in}')\n",
    "print(f'#####################')\n",
    "\n",
    "### step 1: initialization\n",
    "samples = ((np.ptp(x)*np.random.rand(M_sub_samples))-np.max(x)).flatten()  \n",
    "beta=0\n",
    "beta_list = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 1.0]\n",
    "\n",
    "for b in beta_list:\n",
    "    \n",
    "    log_p1 = log_prob(samples,beta,mu,sigma)\n",
    "    beta_new = b\n",
    "    log_p2 = log_prob(samples, beta_new, mu, sigma)\n",
    "    log_w = log_p2 - log_p1\n",
    "    log_w_rel = log_w-np.max(log_w)\n",
    "    w_rel = np.exp(log_w_rel)\n",
    "    p_rel = w_rel/np.sum(w_rel)\n",
    "    print(f'b={b}')\n",
    "    print(f'sum(w_rel) = {np.sum(w_rel)}')\n",
    "    print(f'mean(w_rel) = {np.mean(w_rel)}')\n",
    "\n",
    "    resamples = np.random.choice(samples,size=K_walkers,p=p_rel)\n",
    "    resamples_hist, resamples_edges = np.histogram(resamples, 100, range=(-20,20), density=True)\n",
    "\n",
    "    p0 = np.array([[s] for s in resamples])\n",
    "\n",
    "    sampler = emcee.EnsembleSampler(K_walkers, dim, log_prob, args=[beta_new, mu, sigma])\n",
    "    state = sampler.run_mcmc(p0, N_steps)\n",
    "    samples = (sampler.flatchain[burn_in:]).flatten()\n",
    "    samples_hist, samples_edges = np.histogram(samples, 100, range=(-20,20), density=True)\n",
    "    #assert(np.size(samples)==M_sub_samples)\n",
    "\n",
    "    # reference - affine sampler\n",
    "    p0_ref = (np.ptp(x)*np.random.rand(K_walkers, dim))-np.max(x)\n",
    "    sampler_ref = emcee.EnsembleSampler(K_walkers, dim, log_prob, args=[beta_new, mu, sigma])\n",
    "    state_ref = sampler_ref.run_mcmc(p0_ref, N_steps)\n",
    "    samples_ref = sampler_ref.flatchain[burn_in:]\n",
    "    samples_ref_hist, samples_ref_edges = np.histogram(samples_ref, 100, range=(-20,20), density=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'old beta = {beta}')\n",
    "    print(f'new beta = {beta_new}')\n",
    "    print(f'number of resamples: {np.size(resamples)}')\n",
    "    print(f'number of samples: {np.size(samples)}')\n",
    "    print(f'number of samples (ref): {np.size(samples_ref)}')\n",
    "    print(f'overlapping coefficient (resamples vs samples): {OVL(resamples_hist,samples_hist,resamples_edges,samples_edges)}' )\n",
    "    print(f'overlapping coefficient (resamples vs samples ref): {OVL(resamples_hist,samples_ref_hist,resamples_edges,samples_ref_edges)}' )\n",
    "    print(f'overlapping coefficient (samples vs samples ref): {OVL(samples_hist,samples_ref_hist,samples_edges,samples_ref_edges)}' )\n",
    "    print(f'\\n')\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(f'beta={beta_new}')\n",
    "    #print(np.shape(x))\n",
    "    #print(np.shape(p_rel))\n",
    "    #plt.bar(x, p_rel, color='red', label='rel prob', alpha=0.55)\n",
    "    plt.hist(resamples, 100, range=(-20,20), color=\"green\", histtype=\"step\", density=True, label='resamples', alpha=0.75)\n",
    "    plt.hist(samples, 100, range=(-20,20), color=\"blue\", histtype=\"step\", density=True, label='sub-samples', alpha=0.75)\n",
    "    plt.hist(samples_ref, 100, range=(-20,20), color=\"black\", histtype=\"step\", density=True, label='ref samples', alpha=0.75)\n",
    "    \n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    beta=beta_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868bc84d",
   "metadata": {},
   "source": [
    "# 2D transporter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc71183",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "import tellurium as te\n",
    "import time\n",
    "\n",
    "\n",
    "### 2d transporter\n",
    "\n",
    "def make_2d_prob_grid(n_grid, a_range, b_range, beta, K_true, y_obs, m):\n",
    "    \n",
    "    log_like_ref = calc_grid_point([K_true[0],K_true[1]],y_obs,m)\n",
    "\n",
    "    n_grid = 100\n",
    "\n",
    "    a_vals = np.linspace(a_range[0],a_range[1],n_grid)\n",
    "    b_vals = np.linspace(b_range[0],b_range[1],n_grid)\n",
    "\n",
    "\n",
    "    a_tmp, b_tmp = np.meshgrid(a_vals,b_vals)\n",
    "    Z = np.zeros((n_grid,n_grid))\n",
    "    log_like_max = np.NINF\n",
    "\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            K = [a_tmp[i,j],b_tmp[i,j]]\n",
    "            Z[i,j] = beta*calc_grid_point(K,y_obs,m)  # log probability\n",
    "            #print(K[0],K[1],Z[i,j])\n",
    "            if Z[i,j] > log_like_max:\n",
    "                log_like_max = Z[i,j]\n",
    "\n",
    "    print(f'log-likelihood: ref={log_like_ref}, grid max={log_like_max}')\n",
    "    fig = plt.figure(figsize=(10,10))                    \n",
    "    c = plt.pcolor(a_tmp, b_tmp, Z, shading='auto', vmin=log_like_max-5, vmax=log_like_max)\n",
    "    #fig.colorbar(c)\n",
    "    plt.title(f'log likelihood reference - beta={beta}')\n",
    "    plt.xlabel('p_1')\n",
    "    plt.ylabel('p_2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Normal log-likelihood calculation\n",
    "def calc_norm_log_likelihood(mu,sigma,X):\n",
    "    # Normal log-likelihood function: -[(n/2)ln(2pp*sigma^2)]-[sum((X-mu)^2)/(2*sigma^2)]\n",
    "    # ref: https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood \n",
    "    n = len(X)\n",
    "    f1 = -1*(n/2)*np.log(2*np.pi*sigma**2)\n",
    "    f2_a = -1/(2*sigma**2)\n",
    "    f2_b = 0 \n",
    "    for i in range(n):\n",
    "        f2_b += (X[i]-mu[i])**2\n",
    "    f2 = f2_a*f2_b\n",
    "    log_likelihood = f1+f2\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def calc_grid_point(K,y_obs,m):\n",
    "    m.resetToOrigin()\n",
    "    m.H_out = 5e-8\n",
    "    m.integrator.absolute_tolerance = 1e-18\n",
    "    m.integrator.relative_tolerance = 1e-12\n",
    "    m.k1_f = 10**K[0]\n",
    "    m.k1_r = 10**K[1]\n",
    "    m.k6_r = (m.k1_f*m.k2_f*m.k3_f*m.k4_f*m.k5_f*m.k6_f)/(m.k1_r*m.k2_r*m.k3_r*m.k4_r*m.k5_r)\n",
    " \n",
    "    D_tmp = m.simulate(0, 10, 250, selections=['time', 'rxn4'])\n",
    "    y_tmp = D_tmp['rxn4']\n",
    "    sigma = 1e-13\n",
    "    log_like_tmp = calc_norm_log_likelihood(y_tmp,sigma,y_obs)\n",
    "    return log_like_tmp\n",
    "\n",
    "\n",
    "def log_prior(theta):\n",
    "    '''log of uniform prior distribution'''\n",
    "    p0 = theta[0]\n",
    "    p1 = theta[1]\n",
    "\n",
    "    # if prior is between boundary --> log(prior) = 0 (uninformitive prior)\n",
    "    if (6<p0<12) and (-1<p1<5):\n",
    "        return 0  \n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "\n",
    "def log_prob(theta, y_obs, extra_parameters):\n",
    "    '''log of estimated posterior probability'''\n",
    "    m = extra_parameters[0]\n",
    "    beta = extra_parameters[1]\n",
    "    log_pr = log_prior(theta)\n",
    "    if not np.isfinite(log_pr):\n",
    "        return -np.inf  # ~zero probability\n",
    "    log_prob = log_pr + beta*calc_grid_point(theta, y_obs, m)  # log posterior ~ log likelihood + log prior\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "def set_p0():\n",
    "    '''set initial walker positions'''\n",
    "    log_k1_f = np.random.uniform(6, 12) # log10 rate constant (ref=1e10)\n",
    "    log_k1_r = np.random.uniform(-1, 5)  # log10 rate constant (ref=1e3)    \n",
    "    p0_list_tmp = [              \n",
    "                log_k1_f ,\n",
    "                log_k1_r ,\n",
    "    ]\n",
    "    return p0_list_tmp\n",
    "\n",
    "\n",
    "### model configuration\n",
    "print('model configuration...')\n",
    "antimony_string_SS = f\"\"\"\n",
    "            // Created by libAntimony v2.12.0\n",
    "            model transporter_full()\n",
    "\n",
    "            // Compartments and Species:\n",
    "            compartment vol;\n",
    "            species OF in vol, OF_Hb in vol;\n",
    "            species IF_Hb in vol, IF_Hb_Sb in vol;\n",
    "            species IF_Sb in vol, OF_Sb in vol;\n",
    "            species H_in in vol, S_in in vol;\n",
    "            species $H_out in vol, $S_out in vol;\n",
    "\n",
    "            // Reactions:\n",
    "            rxn1: OF + $H_out -> OF_Hb; vol*(k1_f*OF*H_out - k1_r*OF_Hb);\n",
    "            rxn2: OF_Hb -> IF_Hb; vol*(k2_f*OF_Hb - k2_r*IF_Hb);\n",
    "            rxn3: IF_Hb + S_in -> IF_Hb_Sb; vol*(k3_f*IF_Hb*S_in - k3_r*IF_Hb_Sb);\n",
    "            rxn4: IF_Hb_Sb -> IF_Sb + H_in; vol*(k4_f*IF_Hb_Sb - k4_r*IF_Sb*H_in);\n",
    "            rxn5: IF_Sb -> OF_Sb; vol*(k5_f*IF_Sb - k5_r*OF_Sb);\n",
    "            rxn6: OF_Sb -> OF + $S_out; vol*(k6_f*OF_Sb - k6_r*OF*S_out);\n",
    "            \n",
    "\n",
    "            // Events:\n",
    "            E1: at (time >= 5): H_out = 1e-7, S_out = 1e-3;\n",
    "            \n",
    "\n",
    "            // Species initializations:\n",
    "            H_out = 5e-8;\n",
    "            H_out has substance_per_volume;\n",
    "\n",
    "            H_in = 9.999811082242941e-08;\n",
    "            H_in has substance_per_volume;\n",
    "\n",
    "            S_out = 0.001;\n",
    "            S_out has substance_per_volume;\n",
    "\n",
    "            S_in = 0.0009999811143288836;\n",
    "            S_in has substance_per_volume;\n",
    "\n",
    "            OF = 4.7218452046117796e-09;\n",
    "            OF has substance_per_volume;\n",
    "\n",
    "            OF_Hb = 4.7218452046117796e-09;\n",
    "            OF_Hb has substance_per_volume;\n",
    "\n",
    "            IF_Hb = 4.7218452046117796e-09;\n",
    "            IF_Hb has substance_per_volume;\n",
    "            \n",
    "            IF_Hb_Sb = 4.721756029392908e-08;\n",
    "            IF_Hb_Sb has substance_per_volume;\n",
    "            \n",
    "            IF_Sb = 4.721845204611779e-08;\n",
    "            IF_Sb has substance_per_volume;\n",
    "\n",
    "            OF_Sb = 4.721845204611775e-08;\n",
    "            OF_Sb has substance_per_volume;\n",
    "\n",
    "\n",
    "            // Compartment initializations:\n",
    "            vol = 0.0001;\n",
    "            vol has volume;\n",
    "\n",
    "            // Rate constant initializations:\n",
    "            k1_f = 1e10;\n",
    "            k1_r = 1e3;\n",
    "            k2_f = 1e2;\n",
    "            k2_r = 1e2;\n",
    "            k3_f = 1e7;\n",
    "            k3_r = 1e3;\n",
    "            k4_f = 1e3;\n",
    "            k4_r = 1e10;\n",
    "            k5_f = 1e2;\n",
    "            k5_r = 1e2;\n",
    "            k6_f = 1e3;\n",
    "            k6_r = 1e7;\n",
    "\n",
    "\n",
    "            // Other declarations:\n",
    "            const vol;\n",
    "            const k1_f, k1_r, k2_f, k2_r, k3_f, k3_r;\n",
    "            const k4_f, k4_r, k5_f, k5_r, k6_f, k6_r;\n",
    "    \n",
    "\n",
    "            // Unit definitions:\n",
    "            unit substance_per_volume = mole / litre;\n",
    "            unit volume = litre;\n",
    "            unit length = metre;\n",
    "            unit area = metre^2;\n",
    "            unit time_unit = second;\n",
    "            unit substance = mole;\n",
    "            unit extent = mole;\n",
    "\n",
    "            // Display Names:\n",
    "            time_unit is \"time\";\n",
    "            end\n",
    "\"\"\" \n",
    "           \n",
    "m = te.loada(antimony_string_SS)\n",
    "m.integrator.absolute_tolerance = 1e-18\n",
    "m.integrator.relative_tolerance = 1e-12\n",
    "m.H_out = 5e-8\n",
    "D1 = m.simulate(0, 10, 250, selections=['time', 'rxn4'])\n",
    "y_true = D1['rxn4']\n",
    "\n",
    "noise_stdev_true = 1e-13\n",
    "y_obs = np.genfromtxt(\"data_grid_test3_1exp.csv\")\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(y_obs, 'o', alpha=0.5)\n",
    "plt.plot(y_true)\n",
    "plt.ylim(-2.5e-11, 2.5e-11)\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('t')\n",
    "\n",
    "\n",
    "\n",
    "# beta_test = [1e-6, 1e-4, 0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# for b in beta_test:\n",
    "#     make_2d_prob_grid(n_grid=100, a_range=[6,12], b_range=[-1,5], beta=b, K_true=[10,3], y_obs=y_obs, m=m )\n",
    "\n",
    "print(f'log_prob_ref: {log_probability([10,3], y_obs, [m,1])}')\n",
    "\n",
    "\n",
    "#### sampling settings\n",
    "t0 = time.time()\n",
    "print(f't0 timestamp: {t0}')\n",
    "print('sampling configuration...')\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "dim = 2\n",
    "max_iter = 1e5\n",
    "ESS_min = 500\n",
    "fractional_weight_target = 0.1\n",
    "\n",
    "d_beta = 1e-5\n",
    "N_steps = 100\n",
    "K_walkers = 100\n",
    "NK_total_samples = N_steps*K_walkers\n",
    "burn_in = 1000\n",
    "\n",
    "M_sub_samples = NK_total_samples - burn_in #10*K_walkers\n",
    "assert((M_sub_samples < NK_total_samples) & (M_sub_samples >= 10*K_walkers))\n",
    "\n",
    "print(f'random seed: {seed}')\n",
    "print(f'n dim: {dim}')\n",
    "print(f'max iterations: {max_iter}')\n",
    "print(f'ESS min: {ESS_min}')\n",
    "print(f'd Beta: {d_beta}')\n",
    "print(f'fractional weight target: {fractional_weight_target}')\n",
    "print(f'N steps: {N_steps}')\n",
    "print(f'K walkers: {K_walkers}')\n",
    "print(f'M sub samples: {M_sub_samples}')\n",
    "print(f'NK total samples: {NK_total_samples}')\n",
    "print(f'burn in: {burn_in}')\n",
    "print(f'#####################')\n",
    "\n",
    "\n",
    "### step 1: initialization\n",
    "print('initializing...')\n",
    "beta=0\n",
    "#beta_list = [1e-6, 1e-4, 0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "#beta_list = [1e-6, 1e-4, 0.001, 0.01, 0.1]\n",
    "\n",
    "pos_list=[]\n",
    "for i in range(K_walkers):\n",
    "    p0_list_tmp = set_p0()\n",
    "    pos_list.append(p0_list_tmp)\n",
    "samples = np.asarray(pos_list)\n",
    "assert(np.shape(samples)==(K_walkers, dim))\n",
    "print(f'initial walker shape: {np.shape(samples)}')\n",
    "print(f'p0: {samples[0:2]}')\n",
    "\n",
    "iter_i = 0\n",
    "while (beta <=1) and (iter_i < max_iter):\n",
    "    print(f'i={iter_i}')\n",
    "    \n",
    "    log_like = np.array([log_prob(theta_i,y_obs,[m,1]) for theta_i in samples])\n",
    "    mean_w_rel = 0\n",
    "    iter_j = 0\n",
    "    beta_new = beta\n",
    "    tmp_beta_list = []\n",
    "    \n",
    "    print('finding new beta...')\n",
    "    beta_new = beta_new+d_beta\n",
    "    log_w = (beta_new-beta)*log_like\n",
    "    log_w_rel = log_w-np.max(log_w)\n",
    "    w_rel = np.exp(log_w_rel)\n",
    "    p_rel = w_rel/np.sum(w_rel)\n",
    "    mean_w_rel = np.mean(w_rel)\n",
    "#     print(f'  beta_new={beta_new}')\n",
    "#     print(f'  sum(w_rel) = {np.sum(w_rel)}')\n",
    "#     print(f'  mean(w_rel) = {np.mean(w_rel)}')\n",
    "\n",
    "#     print(f'  p_rel shape: {np.shape(p_rel)}')\n",
    "#     print(f'  p_rel: {p_rel[0:2]}')\n",
    "#     print(f'  max p_rel: {max(p_rel)}')\n",
    "#     print(f'  argmax p_rel: {np.argmax(p_rel)}')\n",
    "    assert not mean_w_rel < fractional_weight_target, f\"mean fractional weight {mean_w_rel} < target {fractional_weight_target} with smallest step size {d_beta}. use smaller step size\"\n",
    "    tmp_beta_list.append(beta_new)\n",
    "    \n",
    "    while (mean_w_rel>=fractional_weight_target) and (iter_j < max_iter) and beta_new <1:\n",
    "\n",
    "        beta_new = beta_new+d_beta\n",
    "        log_w = (beta_new-beta)*log_like\n",
    "        log_w_rel = log_w-np.max(log_w)\n",
    "        w_rel = np.exp(log_w_rel)\n",
    "        p_rel = w_rel/np.sum(w_rel)\n",
    "        mean_w_rel = np.mean(w_rel)\n",
    "#         print(f'  j = {iter_j}')\n",
    "#         print(f'  beta_new={beta_new}')\n",
    "#         print(f'  sum(w_rel) = {np.sum(w_rel)}')\n",
    "#         print(f'  mean(w_rel) = {np.mean(w_rel)}')\n",
    "\n",
    "#         print(f'  p_rel shape: {np.shape(p_rel)}')\n",
    "#         print(f'  p_rel: {p_rel[0:2]}')\n",
    "#         print(f'  max p_rel: {max(p_rel)}')\n",
    "#         print(f'  argmax p_rel: {np.argmax(p_rel)}')\n",
    "        iter_j = iter_j + 1\n",
    "        #print(f'  ')\n",
    "        tmp_beta_list.append(beta_new)\n",
    "    print(f'new beta = {beta_new} after {iter_j} iterations with mean(w_rel) = {mean_w_rel}')\n",
    "\n",
    "    resamples_index = np.random.choice([ i for i in range(len(samples))],size=K_walkers,p=p_rel)\n",
    "    resamples = samples[resamples_index]\n",
    "    resample_x = [i[0] for i in resamples ]\n",
    "    resample_y = [i[1] for i in resamples ]\n",
    "    if beta_new > 0.1:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(f'log likelihood resample - beta={beta_new}')\n",
    "        resamples_hist, resamples_x_edges, resamples_y_edges, resamples_image = plt.hist2d(resample_x, resample_y, 100, density=True, range=([[6, 12], [-1, 5]]))\n",
    "        plt.xlabel('p2')\n",
    "        plt.ylabel('p1')\n",
    "    #plt.colorbar()\n",
    "    #print(f'resample shape: {np.shape(resamples)}')\n",
    "\n",
    "    p0 = np.array([s for s in resamples])\n",
    "    #print(f'resampled p0 shape: {np.shape(p0)}')\n",
    "\n",
    "    print(f'running Affine sampler...')\n",
    "    sampler = emcee.EnsembleSampler(K_walkers, dim, log_prob, args=[y_obs, [m,beta_new]])\n",
    "    state = sampler.run_mcmc(p0, N_steps)\n",
    "    samples = sampler.flatchain[burn_in:]\n",
    "    print(np.shape(samples))\n",
    "    samples_x = [i[0] for i in samples]\n",
    "    samples_y = [i[1] for i in samples]\n",
    "    if beta_new > 0.1:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(f'log likelihood sampled - beta={beta_new}')\n",
    "        samples_hist, samples_x_edges, samples_y_edges, samples_image = plt.hist2d(samples_x, samples_y, 100, density=True, range=([[6, 12], [-1, 5]]))\n",
    "        plt.xlabel('p2')\n",
    "        plt.ylabel('p1')\n",
    "    #plt.colorbar()\n",
    "    #print(f'sample shape: {np.shape(samples)}')\n",
    "    if beta_new > 0.1:\n",
    "        print(f'calculating reference...')\n",
    "        make_2d_prob_grid(n_grid=100, a_range=[6,12], b_range=[-1,5], beta=beta_new, K_true=[10,3], y_obs=y_obs, m=m )\n",
    "    iter_i = iter_i +1\n",
    "    beta=beta_new\n",
    "\n",
    "\n",
    "tf = time.time()\n",
    "print(f'tf timestamp: {tf}')\n",
    "print(f'wall clock: {tf-t0} s')\n",
    "print(f'{(len(beta_list)*NK_total_samples)/(tf-t0)} samples/sec' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1c047",
   "metadata": {},
   "source": [
    "# 12D transporter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30938ebb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats, scipy as sp\n",
    "import seaborn as sns\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "import tellurium as te\n",
    "import time\n",
    "import corner as corner\n",
    "\n",
    "\n",
    "### 12d transporter\n",
    "\n",
    "# Normal log-likelihood calculation\n",
    "def calc_norm_log_likelihood(mu,sigma,X):\n",
    "    # Normal log-likelihood function: -[(n/2)ln(2pp*sigma^2)]-[sum((X-mu)^2)/(2*sigma^2)]\n",
    "    # ref: https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood \n",
    "    n = len(X)\n",
    "    f1 = -1*(n/2)*np.log(2*np.pi*sigma**2)\n",
    "    f2_a = -1/(2*sigma**2)\n",
    "    f2_b = 0 \n",
    "    for i in range(n):\n",
    "        f2_b += (X[i]-mu[i])**2\n",
    "    f2 = f2_a*f2_b\n",
    "    log_likelihood = f1+f2\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def calc_grid_point(K,y_obs,m):\n",
    "    m.resetToOrigin()\n",
    "    m.H_out = 5e-8\n",
    "    m.integrator.absolute_tolerance = 1e-18\n",
    "    m.integrator.relative_tolerance = 1e-12\n",
    "    m.k1_f = 10**K[0]\n",
    "    m.k1_r = 10**K[1]\n",
    "    m.k2_f = 10**K[2]\n",
    "    m.k2_r = 10**K[3]\n",
    "    m.k3_f = 10**K[4]\n",
    "    m.k3_r = 10**K[5]\n",
    "    m.k4_f = 10**K[6]\n",
    "    m.k4_r = 10**K[7]\n",
    "    m.k5_f = 10**K[8]\n",
    "    m.k5_r = 10**K[9]\n",
    "    m.k6_f = 10**K[10]\n",
    "    m.k6_r = (m.k1_f*m.k2_f*m.k3_f*m.k4_f*m.k5_f*m.k6_f)/(m.k1_r*m.k2_r*m.k3_r*m.k4_r*m.k5_r)\n",
    "    try:\n",
    "        D_tmp = m.simulate(0, 10, 250, selections=['time', 'rxn4'])\n",
    "        y_tmp = D_tmp['rxn4']\n",
    "        sigma = 10**K[11]\n",
    "        log_like_tmp = calc_norm_log_likelihood(y_tmp,sigma,y_obs)\n",
    "    except:\n",
    "        y_tmp = np.zeros(250)\n",
    "        sigma = 10**K[11]\n",
    "        log_like_tmp = calc_norm_log_likelihood(y_tmp,sigma,y_obs)\n",
    "    return log_like_tmp\n",
    "\n",
    "\n",
    "def log_prior(theta):\n",
    "    '''log of uniform prior distribution'''\n",
    "    \n",
    "    p1 = theta[0]\n",
    "    p2 = theta[1]\n",
    "    p3 = theta[2]\n",
    "    p4 = theta[3]\n",
    "    p5 = theta[4]\n",
    "    p6 = theta[5]\n",
    "    p7 = theta[6]\n",
    "    p8 = theta[7]\n",
    "    p9 = theta[8]\n",
    "    p10 = theta[9]\n",
    "    p11 = theta[10]\n",
    "    p_sigma = theta[11]\n",
    "\n",
    "    # if prior is between boundary --> log(prior) = 0 (uninformitive prior)\n",
    "    if (np.log10(5e-14)<p_sigma<np.log10(5e-13)) and (6<p1<12) and (-1<p2<5) and (-2<p3<4) and (-2<p4<4) and \\\n",
    "        (3<p5<9) and (-1<p6<5) and (-1<p7<5) and (6<p8<12) and (-2<p9<4) and (-2<p10<4) and (-1<p11<5):\n",
    "        return 0  \n",
    "    else:\n",
    "        return -np.inf\n",
    "    \n",
    "\n",
    "def log_prob(theta, y_obs, extra_parameters):\n",
    "    '''log of estimated posterior probability'''\n",
    "    m = extra_parameters[0]\n",
    "    beta = extra_parameters[1]\n",
    "    log_pr = log_prior(theta)\n",
    "    log_l = calc_grid_point(theta, y_obs, m)\n",
    "    if not np.isfinite(log_pr):\n",
    "        return -np.inf  # ~zero probability\n",
    "    if not np.isfinite(log_l):\n",
    "        return -np.inf  # ~zero probability\n",
    "    else:\n",
    "        log_prob = log_pr + beta*log_l  # log posterior ~ log likelihood + log prior\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "def set_p0():\n",
    "    '''set initial walker positions'''\n",
    "    \n",
    "    log_k1_f = np.random.uniform(6, 12) # log10 rate constant (ref=1e10)\n",
    "    log_k1_r = np.random.uniform(-1,5)  # log10 rate constant (ref=1e3) \n",
    "    log_k2_f = np.random.uniform(-2,4)  # log10 rate constant (ref=1e2)\n",
    "    log_k2_r = np.random.uniform(-2,4)  # log10 rate constant (ref=1e2)\n",
    "\n",
    "    log_k3_f = np.random.uniform(3,9)  # log10 rate constant (ref=1e7) \n",
    "    log_k3_r = np.random.uniform(-1,5)  # log10 rate constant (ref=1e3) \n",
    "    log_k4_f = np.random.uniform(-1,5)  # log10 rate constant (ref=1e3) \n",
    "    log_k4_r = np.random.uniform(6, 12)  # log10 rate constant (ref=1e10)\n",
    "    log_k5_f = np.random.uniform(-2,4)  # log10 rate constant (ref=1e2)\n",
    "    log_k5_r = np.random.uniform(-2,4)   # log10 rate constant (ref=1e2)\n",
    "    log_k6_f = np.random.uniform(-1,5)  # log10  rate constant (ref=1e3)\n",
    "    log_noise_sigma = np.random.uniform(np.log10(5e-14), np.log10(5e-13))\n",
    "    \n",
    "    p0_list_tmp = [        \n",
    "                log_k1_f ,\n",
    "                log_k1_r ,\n",
    "                log_k2_f ,\n",
    "                log_k2_r ,\n",
    "                log_k3_f , \n",
    "                log_k3_r ,\n",
    "                log_k4_f ,\n",
    "                log_k4_r ,\n",
    "                log_k5_f ,\n",
    "                log_k5_r ,\n",
    "                log_k6_f ,\n",
    "                log_noise_sigma ,\n",
    "    ]\n",
    "    return p0_list_tmp\n",
    "\n",
    "\n",
    "def plot_samples(samples, p_info, beta):\n",
    "    \n",
    "    ### 1D posterior plot\n",
    "    samples_T = np.transpose(samples)\n",
    "    fig, ax = plt.subplots(3,4, figsize=(20,15))\n",
    "    axes = ax.flatten()\n",
    "\n",
    "    for i, ax_i in enumerate(axes):\n",
    "        p_tmp = p_info[i]\n",
    "        ax_i.set_title(f\"{p_tmp[0]}\")\n",
    "        ax_i.axvline(p_tmp[3],ls='--', color='black', alpha=0.5)\n",
    "        ax_i.set_xlim(p_tmp[1],p_tmp[2])\n",
    "\n",
    "\n",
    "        ax_i.hist(samples_T[i], 100, histtype=\"step\", density=True, range=(p_tmp[1],p_tmp[2]), label=f'{p_tmp[0]}')\n",
    "        ax_i.legend()\n",
    "\n",
    "    plt.suptitle(f'1D distributions - beta={beta}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #plt.savefig(f'12D_transporter_AIES_PT_1D_dist_s{seed}_random.png') \n",
    "    \n",
    "\n",
    "def calculate_weights(log_like, beta_old, beta_new):\n",
    "    log_w = (beta_new-beta)*log_like\n",
    "    log_w_rel = log_w-np.max(log_w)\n",
    "    w_rel = np.exp(log_w_rel)\n",
    "    return w_rel\n",
    "    \n",
    "\n",
    "def calculate_next_beta(log_like, beta_old, threshold):\n",
    "    def f(x):\n",
    "        avg_w_rel =  np.mean(calculate_weights(log_like, beta_old, x))    \n",
    "        return avg_w_rel-threshold\n",
    "    \n",
    "    beta_new = sp.optimize.root(f, beta_old).x[0]\n",
    "    assert(beta_old <= beta_new)\n",
    "    if beta_new >= 1.0:\n",
    "        beta_new = 1.0\n",
    "    else:\n",
    "        assert(np.isclose(np.mean(calculate_weights(log_like, beta_old, beta_new)),threshold))\n",
    "    return beta_new\n",
    "\n",
    "\n",
    "def calculate_p_rel(log_like, beta_old, beta_new):\n",
    "    w_rel = calculate_weights(log_like, beta, beta_new)\n",
    "    mean_w_rel = np.mean(w_rel)\n",
    "    p_rel = w_rel/np.sum(w_rel)\n",
    "    assert(np.isclose(np.sum(p_rel),1.0))\n",
    "    return p_rel\n",
    "    \n",
    "    \n",
    "### model configuration\n",
    "print('model configuration...')\n",
    "antimony_string_SS = f\"\"\"\n",
    "            // Created by libAntimony v2.12.0\n",
    "            model transporter_full()\n",
    "\n",
    "            // Compartments and Species:\n",
    "            compartment vol;\n",
    "            species OF in vol, OF_Hb in vol;\n",
    "            species IF_Hb in vol, IF_Hb_Sb in vol;\n",
    "            species IF_Sb in vol, OF_Sb in vol;\n",
    "            species H_in in vol, S_in in vol;\n",
    "            species $H_out in vol, $S_out in vol;\n",
    "\n",
    "            // Reactions:\n",
    "            rxn1: OF + $H_out -> OF_Hb; vol*(k1_f*OF*H_out - k1_r*OF_Hb);\n",
    "            rxn2: OF_Hb -> IF_Hb; vol*(k2_f*OF_Hb - k2_r*IF_Hb);\n",
    "            rxn3: IF_Hb + S_in -> IF_Hb_Sb; vol*(k3_f*IF_Hb*S_in - k3_r*IF_Hb_Sb);\n",
    "            rxn4: IF_Hb_Sb -> IF_Sb + H_in; vol*(k4_f*IF_Hb_Sb - k4_r*IF_Sb*H_in);\n",
    "            rxn5: IF_Sb -> OF_Sb; vol*(k5_f*IF_Sb - k5_r*OF_Sb);\n",
    "            rxn6: OF_Sb -> OF + $S_out; vol*(k6_f*OF_Sb - k6_r*OF*S_out);\n",
    "            \n",
    "\n",
    "            // Events:\n",
    "            E1: at (time >= 5): H_out = 1e-7, S_out = 1e-3;\n",
    "            \n",
    "\n",
    "            // Species initializations:\n",
    "            H_out = 5e-8;\n",
    "            H_out has substance_per_volume;\n",
    "\n",
    "            H_in = 9.999811082242941e-08;\n",
    "            H_in has substance_per_volume;\n",
    "\n",
    "            S_out = 0.001;\n",
    "            S_out has substance_per_volume;\n",
    "\n",
    "            S_in = 0.0009999811143288836;\n",
    "            S_in has substance_per_volume;\n",
    "\n",
    "            OF = 4.7218452046117796e-09;\n",
    "            OF has substance_per_volume;\n",
    "\n",
    "            OF_Hb = 4.7218452046117796e-09;\n",
    "            OF_Hb has substance_per_volume;\n",
    "\n",
    "            IF_Hb = 4.7218452046117796e-09;\n",
    "            IF_Hb has substance_per_volume;\n",
    "            \n",
    "            IF_Hb_Sb = 4.721756029392908e-08;\n",
    "            IF_Hb_Sb has substance_per_volume;\n",
    "            \n",
    "            IF_Sb = 4.721845204611779e-08;\n",
    "            IF_Sb has substance_per_volume;\n",
    "\n",
    "            OF_Sb = 4.721845204611775e-08;\n",
    "            OF_Sb has substance_per_volume;\n",
    "\n",
    "\n",
    "            // Compartment initializations:\n",
    "            vol = 0.0001;\n",
    "            vol has volume;\n",
    "\n",
    "            // Rate constant initializations:\n",
    "            k1_f = 1e10;\n",
    "            k1_r = 1e3;\n",
    "            k2_f = 1e2;\n",
    "            k2_r = 1e2;\n",
    "            k3_f = 1e7;\n",
    "            k3_r = 1e3;\n",
    "            k4_f = 1e3;\n",
    "            k4_r = 1e10;\n",
    "            k5_f = 1e2;\n",
    "            k5_r = 1e2;\n",
    "            k6_f = 1e3;\n",
    "            k6_r = 1e7;\n",
    "\n",
    "\n",
    "            // Other declarations:\n",
    "            const vol;\n",
    "            const k1_f, k1_r, k2_f, k2_r, k3_f, k3_r;\n",
    "            const k4_f, k4_r, k5_f, k5_r, k6_f, k6_r;\n",
    "    \n",
    "\n",
    "            // Unit definitions:\n",
    "            unit substance_per_volume = mole / litre;\n",
    "            unit volume = litre;\n",
    "            unit length = metre;\n",
    "            unit area = metre^2;\n",
    "            unit time_unit = second;\n",
    "            unit substance = mole;\n",
    "            unit extent = mole;\n",
    "\n",
    "            // Display Names:\n",
    "            time_unit is \"time\";\n",
    "            end\n",
    "\"\"\" \n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "m = te.loada(antimony_string_SS)\n",
    "m.integrator.absolute_tolerance = 1e-18\n",
    "m.integrator.relative_tolerance = 1e-12\n",
    "m.H_out = 5e-8\n",
    "D1 = m.simulate(0, 10, 250, selections=['time', 'rxn4'])\n",
    "y_true = D1['rxn4']\n",
    "\n",
    "noise_stdev_true = 1e-13\n",
    "y_obs = np.genfromtxt(\"data_grid_test3_1exp.csv\")\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(y_obs, 'o', alpha=0.5)\n",
    "plt.plot(y_true)\n",
    "plt.ylim(-2.5e-11, 2.5e-11)\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('t')\n",
    "\n",
    "p_info = [   \n",
    "    [\"log_k1_f\",6,12,10],\n",
    "    [\"log_k1_r\",-1,5,3],\n",
    "    [\"log_k2_f\",-2,4,2],\n",
    "    [\"log_k2_r\",-2,4,2],\n",
    "    [\"log_k3_f\",3,9,7],\n",
    "    [\"log_k3_r\",-1,5,3],\n",
    "    [\"log_k4_f\",-1,5,3],\n",
    "    [\"log_k4_r\",6,12,10],\n",
    "    [\"log_k5_f\",-2,4,2],\n",
    "    [\"log_k5_r\",-2,4,2],\n",
    "    [\"log_k6_f\",-1,5,3],\n",
    "    [\"log_sigma\",np.log10(5e-14), np.log10(5e-13), -13],\n",
    "]\n",
    "param_ref = [p_i[3] for p_i in p_info]\n",
    "print(param_ref)\n",
    "\n",
    "# beta_test = [1e-6, 1e-4, 0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# for b in beta_test:\n",
    "\n",
    "#     make_2d_prob_grid(n_grid=100, a_range=[6,12], b_range=[-1,5], beta=b, K_true=[10,3], y_obs=y_obs, m=m )\n",
    "\n",
    "print(f'log_prob_ref: {log_prob(param_ref, y_obs, [m,1])}')\n",
    "\n",
    "\n",
    "#### sampling settings\n",
    "t0 = time.time()\n",
    "print(f't0 timestamp: {t0}')\n",
    "print('sampling configuration...')\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "dim = 12\n",
    "max_iter = 1e5\n",
    "fractional_weight_target = 0.999\n",
    "\n",
    "d_beta = 1e-5\n",
    "N_steps = 100\n",
    "K_walkers = 100\n",
    "NK_total_samples = N_steps*K_walkers\n",
    "burn_in = 1000\n",
    "\n",
    "M_sub_samples = NK_total_samples - burn_in #10*K_walkers\n",
    "assert((M_sub_samples < NK_total_samples) & (M_sub_samples >= 10*K_walkers))\n",
    "\n",
    "print(f'random seed: {seed}')\n",
    "print(f'n dim: {dim}')\n",
    "print(f'max iterations: {max_iter}')\n",
    "print(f'd Beta: {d_beta}')\n",
    "print(f'fractional weight target: {fractional_weight_target}')\n",
    "print(f'N steps: {N_steps}')\n",
    "print(f'K walkers: {K_walkers}')\n",
    "print(f'M sub samples: {M_sub_samples}')\n",
    "print(f'NK total samples: {NK_total_samples}')\n",
    "print(f'burn in: {burn_in}')\n",
    "print(f'#####################')\n",
    "\n",
    "\n",
    "### step 1: initialization\n",
    "print('initializing...')\n",
    "beta=0.0\n",
    "\n",
    "pos_list=[]\n",
    "for i in range(K_walkers):\n",
    "    p0_list_tmp = set_p0()\n",
    "    pos_list.append(p0_list_tmp)\n",
    "samples = np.asarray(pos_list)\n",
    "assert(np.shape(samples)==(K_walkers, dim))\n",
    "print(f'initial walker shape: {np.shape(samples)}')\n",
    "print(f'p0: {samples[0:2]}')\n",
    "\n",
    "print('sampling...')\n",
    "iter_i = 0\n",
    "while (beta <1) and (iter_i < max_iter):\n",
    "   \n",
    "\n",
    "    log_like = np.array([log_prob(theta_i,y_obs,[m,1]) for theta_i in samples])\n",
    "    beta_new = calculate_next_beta(log_like, beta, fractional_weight_target)\n",
    "    p_rel = calculate_p_rel(log_like, beta, beta_new)\n",
    "    \n",
    "\n",
    "    resamples_index = np.random.choice([ i for i in range(len(samples))],size=K_walkers,p=p_rel)\n",
    "    resamples = samples[resamples_index]\n",
    "\n",
    "    p0 = np.array([s for s in resamples])\n",
    "    assert(np.shape(p0)==(K_walkers, dim))\n",
    "\n",
    "    \n",
    "    sampler = emcee.EnsembleSampler(K_walkers, dim, log_prob, args=[y_obs, [m,beta_new]])\n",
    "    state = sampler.run_mcmc(p0, N_steps)\n",
    "    samples = sampler.flatchain[burn_in:]\n",
    "    \n",
    "    if beta_new > 0.9:\n",
    "        plot_samples(samples,p_info, beta_new)\n",
    "    \n",
    "    print(f'i={iter_i}, new beta={beta_new:.2g}, old beta={beta:.2g}, delta beta={(beta_new-beta):.2g}')\n",
    "    \n",
    "    iter_i = iter_i +1\n",
    "    beta=beta_new\n",
    "    \n",
    "\n",
    "tf = time.time()\n",
    "print(f'{iter_i} beta steps')\n",
    "print(f'tf timestamp: {tf}')\n",
    "print(f'wall clock: {tf-t0} s')\n",
    "print(f'{(iter_i*NK_total_samples)/(tf-t0)} samples/sec' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83445ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(samples, columns=[i[0] for i in p_info])\n",
    "df.to_csv(f'ais_affine_{K_walkers}w_{N_steps}s_{fractional_weight_target}t_{seed}r.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30663fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613f2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
